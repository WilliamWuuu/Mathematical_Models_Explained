{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林需要调整的参数有：\n",
    "\n",
    "(1) 决策树的个数\n",
    "\n",
    "(2) 特征属性的个数\n",
    "\n",
    "(3) 递归次数(即决策树的深度)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "from numpy import zeros\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 生成数据集。数据集包括标签，全包含在返回值的dataset上\n",
    "def get_Datasets():\n",
    "    dataSet,classLabels=make_classification(n_samples=200,n_features=100,n_classes=2)\n",
    "    print(dataSet.shape,classLabels.shape)\n",
    "    return np.concatenate((dataSet,classLabels.reshape((-1,1))),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分数据集，实现交叉验证。可以利用它来选择决策树个数。但本例没有实现其代码。\n",
    "\n",
    "原理如下：\n",
    "\n",
    "第一步，将训练集划分为大小相同的K份；\n",
    "\n",
    "第二步，我们选择其中的K-1分训练模型，将用余下的那一份计算模型的预测值，这一份通常被称为交叉验证集；\n",
    "\n",
    "第三步，我们对所有考虑使用的参数建立模型并做出预测，然后使用不同的K值重复这一过程。\n",
    "\n",
    "然后是关键，我们利用在不同的K下平均准确率最高所对应的决策树个数作为算法决策树个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练集划分为大小相同的 n_folds 份。\n",
    "def splitDataSet(dataSet, n_folds):     \n",
    "    fold_size = len(dataSet) / n_folds\n",
    "    data_split = []\n",
    "    begin = 0\n",
    "    end = fold_size\n",
    "    for i in range(n_folds):\n",
    "        data_split.append(dataSet[begin:end, :])\n",
    "        begin = end\n",
    "        end += fold_size\n",
    "    return data_split\n",
    "\n",
    "# 构建 n 个子集。\n",
    "def get_subsamples(dataSet, n):\n",
    "    subDataSet = []\n",
    "    for i in range(n):\n",
    "        index = []     #每次都重新选择 k 个索引\n",
    "        for _ in range(len(dataSet)):  # 长度是 k\n",
    "            index.append(np.random.randint(len(dataSet)))  # (0, len(dataSet)) 内的一个整数\n",
    "        subDataSet.append(dataSet[index,:])\n",
    "    return subDataSet\n",
    " \n",
    "# 根据某个特征及值对数据进行分类\n",
    "def binSplitDataSet(dataSet,feature,value):\n",
    "    mat0 = dataSet[np.nonzero(dataSet[:, feature] > value)[0], :]\n",
    "    mat1 = dataSet[np.nonzero(dataSet[:, feature] < value)[0], :]\n",
    " \n",
    "    return mat0, mat1\n",
    "\n",
    "# 计算方差，回归时使用。\n",
    "def regErr(dataSet):\n",
    "    return np.var(dataSet[:, -1]) * np.shape(dataSet)[0]\n",
    " \n",
    "# 计算平均值，回归时使用。\n",
    "def regLeaf(dataSet):\n",
    "    return np.mean(dataSet[:, -1])\n",
    " \n",
    "def MostNumber(dataSet):\n",
    "    len0 = len(np.nonzero(dataSet[:,-1]==0)[0])\n",
    "    len1 = len(np.nonzero(dataSet[:,-1]==1)[0])\n",
    "    if len0 > len1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "# 计算基尼指数：一个随机选中的样本在子集中被分错的可能性，是被选中的概率乘以被分错的概率。\n",
    "def gini(dataSet):\n",
    "    corr = 0.0\n",
    "    for i in set(dataSet[:, -1]):           #i 是这个特征下的 某个特征值\n",
    "        corr += (len(np.nonzero(dataSet[:, -1]==i)[0]) / len(dataSet))**2\n",
    "    return 1 - corr\n",
    " \n",
    "def select_best_feature(dataSet, m, alpha=\"huigui\"):\n",
    "    f = dataSet.shape[1]                                                 # 拿过这个数据集，看这个数据集有多少个特征，即f个\n",
    "    index = []\n",
    "    bestS = inf\n",
    "    bestfeature = 0\n",
    "    bestValue = 0\n",
    "    \n",
    "    if alpha==\"huigui\":\n",
    "        S = regErr(dataSet)\n",
    "    else:\n",
    "        S = gini(dataSet)\n",
    "        \n",
    "    for _ in range(m):\n",
    "        index.append(np.random.randint(f))                             # 在f个特征里随机，注意是随机！选择m个特征，然后在这m个特征里选择一个合适的分类特征。 \n",
    "                                                                  \n",
    "    for feature in index:\n",
    "        for splitVal in set(dataSet[:, feature]):                      # set() 函数创建一个无序不重复元素集，用于遍历这个特征下所有的值\n",
    "            mat0, mat1 = binSplitDataSet(dataSet, feature, splitVal)  \n",
    "            \n",
    "            if alpha == \"huigui\":  \n",
    "                newS = regErr(mat0) + regErr(mat1)                     # 计算每个分支的回归方差\n",
    "            else:\n",
    "                newS = gini(mat0) + gini(mat1)                         # 计算被分错率\n",
    "            \n",
    "            if bestS > newS:\n",
    "                bestfeature = feature\n",
    "                bestValue = splitVal\n",
    "                bestS = newS                      \n",
    "    \n",
    "    if (S - bestS) < 0.001 and alpha==\"huigui\":                        # 对于回归来说，方差足够了，那就取这个分支的均值\n",
    "        return None, regLeaf(dataSet)\n",
    "    \n",
    "    elif (S - bestS) < 0.001:\n",
    "        return None, MostNumber(dataSet)                               # 对于分类来说，被分错率足够下了，那这个分支的分类就是大多数所在的类。\n",
    "    \n",
    "    # mat0,mat1=binSplitDataSet(dataSet,feature,splitVal)\n",
    "    return bestfeature,bestValue\n",
    " \n",
    "def createTree(dataSet, alpha=\"huigui\", m=20, max_level=10):           # 实现决策树，使用20个特征，深度为10，\n",
    "    bestfeature, bestValue = select_best_feature(dataSet, m, alpha=alpha)\n",
    "    \n",
    "    if bestfeature==None:\n",
    "        return bestValue\n",
    "    \n",
    "    retTree = {}\n",
    "    max_level -= 1\n",
    "    \n",
    "    if max_level < 0:   #控制深度\n",
    "        return regLeaf(dataSet)\n",
    "    retTree['bestFeature'] = bestfeature\n",
    "    retTree['bestVal'] = bestValue\n",
    "    lSet, rSet = binSplitDataSet(dataSet, bestfeature, bestValue)      # lSet 是根据特征 bestfeature 分到左边的向量，rSet 是根据特征 bestfeature 分到右边的向量\n",
    "    retTree['right'] = createTree(rSet, alpha, m, max_level)\n",
    "    retTree['left'] = createTree(lSet, alpha, m, max_level)            # 每棵树都是二叉树，往下分类都是一分为二。\n",
    "    return retTree\n",
    " \n",
    "def RondomForest(dataSet, n, alpha=\"huigui\"):   # 树的个数\n",
    "    Trees = []\n",
    "    for _ in range(n):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(dataSet[:, :-1], dataSet[:, -1], test_size=0.33, random_state=42)\n",
    "        X_train = np.concatenate((X_train, y_train.reshape((-1,1))), axis=1)\n",
    "        Trees.append(createTree(X_train, alpha=alpha))\n",
    "    return Trees\n",
    "\n",
    "def treeForecast(trees, data, alpha=\"huigui\"):      \n",
    "    if alpha==\"huigui\":\n",
    "        if not isinstance(trees, dict):                                 # isinstance() 函数来判断一个对象是否是一个已知的类型\n",
    "            return float(trees)\n",
    "        \n",
    "        # 如果数据的这个特征大于阈值，那就调用左支\n",
    "        if data[trees['bestFeature']] > trees['bestVal']:     \n",
    "            # 如果左支已经是节点了，就返回数值。          \n",
    "            if type(trees['left'])=='float':                              \n",
    "                return trees['left']\n",
    "            # 如果左支还是字典结构，那就继续调用， 用此支的特征和特征值进行选支。           \n",
    "            else:\n",
    "                return treeForecast(trees['left'], data, alpha)\n",
    "        else:\n",
    "            if type(trees['right'])=='float':\n",
    "                return trees['right']\n",
    "            else:\n",
    "                return treeForecast(trees['right'], data, alpha)   \n",
    "    \n",
    "    else:\n",
    "        if not isinstance(trees, dict):                                \n",
    "            return int(trees)\n",
    "        \n",
    "        if data[trees['bestFeature']] > trees['bestVal']:\n",
    "            if type(trees['left'])=='int':\n",
    "                return trees['left']\n",
    "            else:\n",
    "                return treeForecast(trees['left'], data, alpha)\n",
    "        else:\n",
    "            if type(trees['right'])=='int':\n",
    "                return trees['right']\n",
    "            else:\n",
    "                return treeForecast(trees['right'], data, alpha)   \n",
    "            \n",
    "# 随机森林对数据集打上标签\n",
    "def createForeCast(trees, test_dataSet, alpha=\"huigui\"):\n",
    "    cm = len(test_dataSet)                      \n",
    "    yhat = np.mat(zeros((cm, 1)))\n",
    "    for i in range(cm):                                     #\n",
    "        yhat[i,0] = treeForecast(trees, test_dataSet[i, :], alpha)    #\n",
    "    return yhat\n",
    " \n",
    " \n",
    "# 随机森林预测\n",
    "def predictTree(Trees, test_dataSet, alpha=\"huigui\"):\n",
    "    cm = len(test_dataSet)   \n",
    "    yhat = np.mat(zeros((cm, 1)))   \n",
    "    for trees in Trees:\n",
    "        yhat += createForeCast(trees, test_dataSet, alpha)\n",
    "    if alpha==\"huigui\":\n",
    "        yhat /= len(Trees)            # 如果是回归的话，每棵树的结果应该是回归值，相加后取平均\n",
    "    else:\n",
    "        for i in range(len(yhat)): \n",
    "            # 如果是分类的话，每棵树的结果是一个投票向量，相加后，看每类的投票是否超过半数，超过半数就确定为1                                        \n",
    "            if yhat[i, 0] > len(Trees) / 2:            \n",
    "                yhat[i, 0] = 1\n",
    "            else:\n",
    "                yhat[i, 0] = 0\n",
    "    return yhat\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    dataSet = get_Datasets()  \n",
    "    print(dataSet[:, -1].T)                                        # 打印标签，与后面预测值对比  .T其实就是对一个矩阵的转置\n",
    "    RomdomTrees = RondomForest(dataSet, 4, alpha=\"fenlei\")         # 这里我训练好了 很多树的集合，就组成了随机森林。一会一棵一棵的调用。\n",
    "    print(\"---------------------RomdomTrees------------------------\")\n",
    "    test_dataSet = dataSet                                         # 得到数据集和标签\n",
    "    yhat = predictTree(RomdomTrees, test_dataSet, alpha=\"fenlei\")  # 调用训练好的那些树。综合结果，得到预测值。\n",
    "    print(yhat.T)\n",
    "    print(dataSet[:, -1].T-yhat.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sxjm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
